---
title: "Car data"
author: "Enrique Pérez & Jialian Zhou"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    theme: cosmo
    highlight: tango
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r library, include=FALSE, warning=FALSE, message=FALSE}
library(readxl)
library(GGally)
library(mice)
library(rrcov)
library(corrplot)
library(factoextra)
library(cluster)
library(mltools)
library(data.table)
library(mclust)
library(dbscan)
library(MASS)
library(moments)
library(class)
library(naivebayes)
library(nnet)
```

```{r dataread, include=FALSE, warning=FALSE}
#setwd("C:/Users/lucia/Desktop/Big Data Analysis/2º/Statistical learning/project")
setwd("C:/Users/enriq/Desktop/UNIVERSIDAD/Máster en Big Data/2ª Bimestre/Statistical learning/Project")
data = read_excel("Cardata.xlsx", col_types = c("numeric", 
     "text", "text", "text", "text", "text", 
     "text", "text", "numeric", "numeric", 
     "numeric", "numeric", "numeric", "text", 
     "text", "numeric", "text", "numeric", 
     "numeric", "numeric", "numeric", "numeric", 
     "numeric", "numeric", "numeric"), na = "?")

data$symboling = as.factor(data$symboling)
data$`fuel_type` = as.factor(data$`fuel_type`)
data$aspiration = as.factor(data$aspiration)
data$`num_of_doors` = as.factor(data$`num_of_doors`)
data$`body_style` = as.factor(data$`body_style`)
data$`drive_wheels` = as.factor(data$`drive_wheels`)
data$`engine_location` = as.factor(data$`engine_location`)
data$`engine_type` = as.factor(data$`engine_type`)
data$`num_of_cylinders` = as.factor(data$`num_of_cylinders`)
data$make = as.factor(data$make)
data$`fuel_system` = as.factor(data$`fuel_system`)
```


# Introduction

The dataset used in this project contains the following information about 205 cars:

-	symboling (risk level): categorical variable -3, -2, -1, 0, 1, 2, 3. 
-	make (Brand of the car): categorical variable (alfa-romeo, audi, bmw, chevrolet, dodge, honda, isuzu, jaguar, mazda, mercedes-benz, mercury, mitsubishi, nissan, peugeot, plymouth, porsche, renault, saab, subaru, toyota, volkswagen, volvo)
-	fuel_type: binary variable (diesel, gas)
-	aspiration (engine’s aspiration): binary variable (std, turbo)
-	num_of_doors (number of doors): binary variable (four, two)
-	body_style (type of car): categorical variable (hardtop, wagon, sedan, hatchback, convertible)
-	drive_wheels (drive wheels): categorical variable (4wd, fwd, rwd)
-	engine_location: binary variable (front, rear).
-	wheel_base (distance between the wheels): numeric variable which values are between 86.6 and 120.9.
-	length: numeric variable which values are between 141.1 and 208.1.
-	width: numeric variable which values are between 60.3 and 72.3.
-	height: numeric variable which values are between 47.8 and 59.8.
-	curb_weight: numeric variable which values are between 1488 and 4066.
-	engine_type: categorical variable (dohc, dohcv, l, ohc, ohcf, ohcv, rotor).
-	num_of_cylinders (number of cylinders): categorical variable (eight, five, four, six, three, twelve, two).
-	engine_size: numeric variable which values are between 61 and 326.
-	fuel_system: categorical variable (1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi).
-	bore (engine’s diameter): numeric variable which values are between 2.54 and 3.94.
-	stroke: numeric variable which values are between 2.07 and 4.17.
-	compression_ratio: numeric variable which values are between 7 and 23.
-	horsepower: numeric variable which values are between 48 and 288.
-	peak_rpm (maximum revolutions per minute): numeric variable which values are between 4150 and 6600.
-	city_mpg: numeric variable which values are between 13 and 49.
-	highway_mpg: numeric variable which values are between 16 and 54.
-	price: numeric variable which values are between 5118 and 45400.

All the variables are meassured in the United States’ units such as miles, dollars, inches, …

```{r show, warning=FALSE}
skimr::skim(data)
```

In the previous summary of  the data, we can see a big difference in the values from the numerical variables as the data is not normalized. Therefore, we can expect the variances to be quite different just because of the  units of measurements. This has to be taken into consideration before carrying out any dimension reduction analysis. 

Moreover, there are a few missing values in the following variables: num_of_doors, bore, stroke, horsepower, peak_rpm and price. As this absence may difficult the study, we will perform an imputation based on predictive mean matching for these variables.

The predictive mean matching will complete the observations using a linear regression of the missing value using the rest of the variables. The estimation of the predictors is accomplished with the covariance matrix.

$$ x_j = \beta_0 + \beta_1 * x_1  + ... + \beta_p * x_p + \epsilon_j $$

```{r, warning=FALSE,message=FALSE}
data_imp = mice(data,method="pmm", seed = 100407485)
data_imp = complete(data_imp)
data <- data_imp
```


# Data anlaysis

The main characteristics of the quantitative variables are: 

```{r}
# Mean vector
m = colMeans(data[,c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "city_mpg", "highway_mpg", "price")], na.rm = T)
```

$$ \overline{x} = (98.76, 174.05, 65.91, 53.72,  2555.57, 126.91, 3.33 ) $$

That is to say that mean measurements of the cars that are in this data set are:

- Wheel base:  98.76 inches.          
- Length: 174.05 inches.            
- Width: 65.91 inches.            
- Height:  53.72 inches.          
- Curb weight: 2555.57 pounds.         

This cars also have an engine size with a mean of 126.91 cubic inches, while the mean of the bore and stroke is of 3.33 and 3.26 inches respectively. The mean of the compression ratio is 10.14. In addition, the vehicles have mean of the horsepower of 104.26, with the mean of the peak revolutions per minute of 5125.37 revolutions, and the mean consumption in the city and in the highway are 17.12 and 30.75 miles per gallon each. 

These characteristics make the cars' value to be around 13207.13$. 

Before, calculating the covariance and the correlation matrixes, we will eliminate the `city_mpg` variable as it is almost constant and it interferes with the study.

```{r}
data <- data[,!names(data) %in% c("city_mpg")]
```


```{r}
# Covariance matrix
S = var(data[,c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price")], na.rm = T)
round(S, 2)
```

As it was expected, the values from the variances differ a lot between each variable due to the measurements. The variable with the highest variance is the price.

```{r}
# Correlation matrix
R = cor(data[,c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price")], use="complete.obs")
round(R, 2)
corrplot(R,order="hclust")
```

These correlation matrix show a positive linear relation between the variables that indicate the dimension's of the car and the ones related to the power of the engine. In addition, these two groups are correlated to the price positively. Therefore, we can say that the most expensive cars will be the bigger ones with a powerful engine. On the other hand, we can see that the bigger and more expensive the car, the less miles per gallon (they consume more).


A scatterplot matrix with every numeric variable is shown, in order to carry out the first graphical analysis.

```{r pairs, warning=FALSE}
ggpairs(data,         
        columns = c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price"), 
        upper = list(continuous = wrap("cor", size = 1.5)),
        lower = list(continuous = wrap("points", size = 0.5)))  + 
  theme(strip.text.x = element_text(size = 4),
        strip.text.y = element_text(size = 2),
        axis.text.x = element_text(size = 2),
        axis.text.y = element_text(size = 2))
```

In the plot, it can be seen that the `length` and the `peak_rpm` of the car are the only variables which are not heavily skewed. All the other variables are right skewed (the mean is larger than the median), except for the `height`, the `bore` and the `stroke` which are left skewed (the mean is smaller than the median).

Moreover, a correlation analysis can be carried out with this plot:

- Positive correlation between the `wheel_base`, the `length`, the `width`, the `curb_weight` and the `engine_size` of the car: The vehicles with a large wheel base will be the bigger ones as they will also be longer, wider and heavier. 

- Positive correlation between the `engine_size`, the `horsepower` and the `price` of the car: the bigger cars are usually the ones with a the most powerful engines. Therefore, the price are also higher.

- The `price` has a positive correlation with the `width`, the `curb_weight`, the `engine_size` and the `horsepower` but a negative correlation with the `highway_mpg`: The bigger cars are the ones that consume the most fuel int he highway.


## Group differenciation

### `Fuel_type`

The main characteristics for each group are calculated in order to get a first idea of the differences these groups may have. 

We have to take into account that there is a big difference in the sample sizes for each group: there are only 20 cars fueled by diesel while there are 185 cars powered by gas. Therefore, the conclusions may not be reliable.

```{r}
# Mean vector
m_fuel = aggregate(x = data[,c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price")],               
          by = list(data$`fuel_type`),              
          FUN = mean)

m_fuel
```

After comparing both mean vectors, we can say that the cars fueled by gas are usually smaller but more powerful and environmentally friendly. However, the engine size does not seem different at first, we would have to carry out an hypothesis test to check the existance of this difference. Moreover, the gas cars have are cheaper if we look at the mean, which may be caused due to the size as they are smaller.

```{r, warning=FALSE}
# Covariance matrix
data_num = data[,c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price")]
data_gas = data_num[data$`fuel_type`=="gas",]
data_diesel = data_num[data$`fuel_type`=="diesel",]

(S_diesel = cov(data_diesel))
(S_gas =  cov(data_gas))
```

```{r, warning = FALSE}
# Correlation matrix

(R_diesel = cor(data_diesel))
(R_gas =  cor(data_gas))
```

In other to perform a better analysis, the fuel type will be taken into consideration.  

```{r pairsfuel, warning=FALSE}
ggpairs(data, 
        aes(color = `fuel_type`),
        columns = c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price"), 
        upper = list(continuous = wrap("cor", size = 1.5)),
        lower = list(continuous = wrap("points", size = 0.5)))  + 
  theme(strip.text.x = element_text(size = 4),
        strip.text.y = element_text(size = 2),
        axis.text.x = element_text(size = 2),
        axis.text.y = element_text(size = 2))
```

In this case, the plot shows that the compression ratio will distinguish the cars' fuel type almost perfectly as we can see that those vehicles with a smaller compression ratio will be powered by gas while the ones with a higher compression ratio will be fueled by diesel. 

In addition, the `wheel_base`, the `width`, the `height`, the `curb_weight` and the `highway_mpg` will help in order to difference the car's fuel as the larger cars are usually powered by diesel. 

The peak revolution per minute (`peak_rpm`) is also a good indicator as the diesel cars usually cannot reach the higher revolutions.


### Multicategorical variables.

### `Symboling`

The `symboling` indicates the security of the car (being $-2$ the value of the safest cars and $3$ the most dangerous cars), hence we considered that it will also be a good variable to analyze.

```{r}
# Mean vector
m_symb = aggregate(x = data[,c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price")],               
          by = list(data$symboling),              
          FUN = mean)

m_symb
```

In this case, we can see that the safest cars have the largest means when it comes to the size, but the cars with the biggest mean in the horsepower variable are the most dangerous. Moreover, the mean of the price of the safest cars is not the largest as it is the one from the most dangerous cars.

Before analyzing the variable, we will be gathering the cars in three groups as there are groups with very few instances:

- Risky: symboling = 3 and 2.
- Neutral: symboling = 0 and 1.
- Safe: symboling = -1 and -2


```{r, warning=FALSE}
# Covariance matrix
data_num = data[,c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price")]

data_r = data_num[(data$symboling==3) | (data$symboling==2),]
data_n = data_num[(data$symboling==0) | (data$symboling==1),]
data_s = data_num[(data$symboling==-1) | (data$symboling==-2),]

(S_r = cov(data_r))
(S_n = cov(data_n))
(S_s = cov(data_s))
```


```{r, warning = FALSE}
# Correlation matrix

(R_r = cor(data_r))
(R_n = cor(data_n))
(R_s = cor(data_s))
```

We plot the results in order to analyze them better.

```{r, warning=FALSE}
data$risk <- ifelse(data$symboling==3, 1, 
                    ifelse(data$symboling==2, 1, 
                           ifelse(data$symboling==1, 0, 
                                  ifelse(data$symboling==0, 0, 
                                         ifelse(data$symboling==-1, -1, 
                                                ifelse(data$symboling==-2, -1, 0))))))
data$risk <- as.factor(data$risk)
  
ggpairs(data, 
        aes(color = risk),
        columns = c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price"), 
        upper = list(continuous = wrap("cor", size = 1.5)),
        lower = list(continuous = wrap("points", size = 0.5)))  + 
  theme(strip.text.x = element_text(size = 4),
        strip.text.y = element_text(size = 2),
        axis.text.x = element_text(size = 2),
        axis.text.y = element_text(size = 2))
```

In the plot, it can be seen that the least secure cars are usually the smallest but they are not the cheapest. However, the safest cars are not the largest either but they usually are in the middle when in comes to the `curb_weight` and the ones with the largest bores without a very high nor very low peak of the revolutions per minute. 

For the most part, the risk of the cars cannot be predicted easily with the plot.

# Outlier's analysis.

The values that are very different from the rest of the data set are considered outliers. 

It is important to study these observations and have them identifed as they can cause a lot of problems in various studies, such as the dimension reduction, due to their influence on the mean vector, the covariance and the correlation matrixes.

In order to determine the existence of outliers in the data set, we are going to estimate the mean vector and the covariance and correlation matrixes with the minimum covariance determinant (MCD) which is based on the Mahalanobis distance:

$$ D_M (x, \mu) = \sqrt{(x- \mu)' \Sigma^{-1} (x- \mu)'}$$

As we have two well-differentiated groups by the `fuel_type` in our data, we will be searching the outliers in each group.

## Gas

```{r}
MCD_est = CovMcd(data_gas,alpha=0.85,nsamp="deterministic")

# Robust mean vector
m_MCD = MCD_est$center
m_MCD

# Robust covariance matrix
S_MCD = MCD_est$cov
S_MCD

# Robust correlation matrix
R_MCD = cov2cor(S_MCD)
R_MCD
```

In the graph shown below, we can see that the first eigenvalue of the covariance matrix obtained with the data is larger that the one calculated with MCD. The rest are more similar.

```{r}
# Compare eigenvalues of both covariance matrices
eval_S = eigen(S)$values
eval_S_MCD = eigen(S_MCD)$values

min_y = min(cbind(eval_S,eval_S_MCD)) - 1
max_y = max(cbind(eval_S,eval_S_MCD)) + 1

plot(1:13,eval_S,col="blue",type="b",xlab="Number",ylab="Eigenvalues",pch=19,
     ylim=c(min_y,max_y),main="Comparison of eigenvalues")
points(1:13,eval_S_MCD,col="red",type="b",pch=19)
legend(7,60000000, legend=c("Eigenvalues of S","Eigenvalues of S MCD"),
       col=c("Blue","Red"),lty=1,cex=1.2)
```

A comparison between the correlation matrixes can also help in order to know the influence of the outliers. 

```{r, warning=FALSE, message=FALSE}
# Compare eigenvalues of both correlation matrices
par(mfrow=c(1,2))
corrplot(R)
corrplot(R_MCD)
```

In this case, we can see that there is not a big difference in the correlations obtained with the data and through the MCD estimation, but we can see that there are some that have been lowered such as `horsepower` and `wheel_base` and `bore` and `width`. 

A graphical analysis has been carried out in order to detect the outliers. In this graphs we are representing the Mahalanobis distances, and the outliers are graphed in green.

```{r}
# Show the squared Mahalanobis distances with the final set of non_outliers
n = nrow(data_gas)
p = ncol(data_gas)

X_sq_Mah_MCD = MCD_est$mah
col_outliers_Mah_MCD = rep("black",n)
outliers_Mah_MCD = which(X_sq_Mah_MCD>qchisq(.99^(1/n),p))
outliers_Mah_MCD
col_outliers_Mah_MCD[outliers_Mah_MCD] = "green"
par(mfrow=c(1,2))
plot(1:n,X_sq_Mah_MCD,pch=19,col=col_outliers_Mah_MCD,
     main="Squared Mahalanobis distances",xlab="Observation",ylab="Squared Mahalanobis distance")
abline(h=qchisq(.99^(1/n),p),lwd=3,col="blue")
plot(1:n,log(X_sq_Mah_MCD),pch=19,col=col_outliers_Mah_MCD,
     main="Log of squared Mahalanobis distances",
     xlab="Observation",ylab="Log of squared Mahalanobis distance")
abline(h=log(qchisq(.99^(1/n),p)),lwd=3,col="blue")
```

```{r, warning=FALSE, message=FALSE}
# Show potential outliers

ggpairs(data_gas, mapping =  aes(color = col_outliers_Mah_MCD),       
        columns = c("wheel_base", "length", "width", "height", "curb_weight", "engine_size", "bore", "stroke", "compressio_ratio", "horsepower", "peak_rpm", "highway_mpg", "price"), 
        upper = list(continuous = wrap("cor", size = 1.5)),
        lower = list(continuous = wrap("points", size = 0.5)))  + 
  theme(strip.text.x = element_text(size = 4),
        strip.text.y = element_text(size = 2),
        axis.text.x = element_text(size = 2),
        axis.text.y = element_text(size = 2))
```

After representing the outliers in the dispersion matrix, we can see that the cars that are considered outliers are those larger, heavier and especially those that consume the most.

## Diesel

```{r, warning=FALSE}
MCD_est = CovMcd(data_diesel,alpha=0.85,nsamp="deterministic")

# Robust mean vector
m_MCD = MCD_est$center
m_MCD

# Robust covariance matrix
S_MCD = MCD_est$cov
S_MCD

# Robust correlation matrix
R_MCD = cov2cor(S_MCD)
R_MCD
```

In the graph shown below, we can see that the first eigenvalue of the covariance matrix obtained with the data is smaller that the one calculated with MCD. The rest are more similar.

```{r}
# Compare eigenvalues of both covariance matrices
eval_S = eigen(S)$values
eval_S_MCD = eigen(S_MCD)$values

min_y = min(cbind(eval_S,eval_S_MCD)) - 1
max_y = max(cbind(eval_S,eval_S_MCD)) + 1

plot(1:13,eval_S,col="blue",type="b",xlab="Number",ylab="Eigenvalues",pch=19,
     ylim=c(min_y,max_y),main="Comparison of eigenvalues")
points(1:13,eval_S_MCD,col="red",type="b",pch=19)
legend(7,60000000, legend=c("Eigenvalues of S","Eigenvalues of S MCD"),
       col=c("Blue","Red"),lty=1,cex=1.2)
```

A comparison between the correlation matrixes can also help in order to know the influence of the outliers. 

```{r, warning=FALSE, message=FALSE}
# Compare eigenvalues of both correlation matrices
par(mfrow=c(1,2))
corrplot(R)
corrplot(R_MCD)
```

In this case, we can see that there is a big difference in the correlations obtained with the data and through the MCD estimation, but it is not a reliable study as there are only 20 cars fueled by diesel and 13 predictors, so the number of observations is not large enough to draw conclusions.

A graphical analysis has been carried out in order to detect the outliers. In this graphs we are representing the Mahalanobis distances, and the outliers are graphed in green.

```{r}
# Show the squared Mahalanobis distances with the final set of non_outliers
n = nrow(data_diesel)
p = ncol(data_diesel)

X_sq_Mah_MCD = MCD_est$mah
col_outliers_Mah_MCD = rep("black",n)
outliers_Mah_MCD = which(X_sq_Mah_MCD>qchisq(.99^(1/n),p))
outliers_Mah_MCD
col_outliers_Mah_MCD[outliers_Mah_MCD] = "green"
par(mfrow=c(1,2))
plot(1:n,X_sq_Mah_MCD,pch=19,col=col_outliers_Mah_MCD,
     main="Squared Mahalanobis distances",xlab="Observation",ylab="Squared Mahalanobis distance")
abline(h=qchisq(.99^(1/n),p),lwd=3,col="blue")
plot(1:n,log(X_sq_Mah_MCD),pch=19,col=col_outliers_Mah_MCD,
     main="Log of squared Mahalanobis distances",
     xlab="Observation",ylab="Log of squared Mahalanobis distance")
abline(h=log(qchisq(.99^(1/n),p)),lwd=3,col="blue")
```

In this plot, we can see that there might be one outlier among the diesel cars.


# Dimension reduction

## Principle Component Analysis.

As we saw in the previous section, most of the variables were highly skewed, hence we are going to apply some transformations in order to make them more symmetrical as the Principle Components Analysis assumes normality in the data.

```{r}
data_pca <- data_num

data_pca$wheel_base <- data_num$wheel_base
data_pca$length <- data_num$length
data_pca$width <- sqrt(data_num$width)
data_pca$height <- sqrt(data_num$height)
data_pca$curb_weight <- data_num$curb_weight
data_pca$engine_size <- log(data_num$engine_size)
data_pca$bore <- (data_num$bore)^2
data_pca$stroke <- (data_num$stroke)^2
data_pca$compressio_ratio <- log(data_num$compressio_ratio)
data_pca$horsepower <- log(data_num$horsepower)
data_pca$peak_rpm <- log(data_num$peak_rpm)
data_pca$highway_mpg <- (data_num$highway_mpg)
data_pca$price <- log(data_num$price)
```


We calculate the main characteristics of the transformed data. 

```{r}
(m = colMeans(data_pca, na.rm = T))

(S_pca <- cov(data_pca))

(R_pca <- cor(data_pca))
```

We can see that the scale of variances is very different, so we are calculating PCA based on the correlation matrix.

```{r}
p1 <- prcomp(data_pca, scale = TRUE)
```

```{r}
# Explained variability
fviz_eig(p1,addlabels = T,ylim=c(0,60))
```

We can see that with the first 3 components, we are able to explain 77.9% of the variance of the original data. Therefore, we are only representing graphically these three components.

The correlation between the original data and the first three components is calculated in order to explain the components and how they behave. Moreover, this correlations will be shown graphically with the help of biplots.

```{r}
cor(data_pca, p1$x[,1:3])
```

```{r}
par(mfrow = c(1,3))
biplot(p1, main="PC1 vs PC2", cex=0.7)
biplot(p1, choices=c(1,3), main="PC1 vs PC3", cex=0.7) 
biplot(p1, choices=c(3,2), main="PC2 vs PC3", cex=0.7)
```

The first component has high positive correlations with the variables `wheel_base`, `length`, `width`, `curb_weight`, `engine_size`, `bore`, `horsepower` and `price` but it has a negative correlation with the `highway_mpg`. This means that the big, non environmentally friendly cars with a lot of power will be on the right side and the smaller cars with less consumption will be on the left. 

The second component has a negative correlation with the height and `compressio_ratio` and a positive correlation with `peak_rpm`. This means that the cars which are tall and more efficient will be at the bottom of the graph while the shorter and less efficient cars will be at the top (PC1 vs PC2).  

The third component has a negative correlation with `stroke`, which indicates the engine's displacement. The cars on the right side will have a longer engine displacement (PC3 vs PC2). Moreover, we can see that there is also positive correlation with the variable `height`, so the taller cars will be displayed on the left side of the biplot (PC3 vs PC2).

The representation of the first 3 components is shown below.

```{r}
color_1 <- "deepskyblue2"
color_2 <- "darkorchid2"
colors_X <- c(color_1,color_2)[1*(data$fuel_type=="gas")+1]
pairs(p1$x[,1:3],col=colors_X,pch=19,main="The first three PCs")
```

We can see that the $2^{nd}$ component is the most useful to identify both groups, being the cars fueled by diesel in blue and the cars fueled by gas in purple. Therefore, we can say that the diesel cars are usually shorter and more efficient and the gas cars are taller and less efficient. This conclusion verifies the assumptions made in the previous sections.


## Independent component analysis

In this case, we are going to transform the numeric data linearly in order to obtain a new matrix, Z, with maximally independent and non-Gaussian variables. 

The way this is achieved is by maximizing the negative entropy.

```{r}
# Matrix dimension
n <- nrow(data_pca)
p <- ncol(data_pca)

# ICs after data transformation
X_trans_ica <- ica::icafast(data_pca,nc=p,alg="par")

# The ICA scores
Z <- X_trans_ica$S
colnames(Z) <- sprintf("IC-%d",seq(1,13))

# Re-scale to have S_z=I 
Z <- Z * sqrt((n-1)/n)

# Histograms of the ICA scores obtained
par(mar=c(1,1,1,1))
par(mfrow=c(4,4))
sapply(colnames(Z),function(cname){hist(as.data.frame(Z)[[cname]],
                                         main=cname,col=color_1,xlab="")})

# Compute the neg-entropy of the columns in Z and sort them in decreasing order of neg-entropy

neg_entropy <- function(z){1/12 * mean(z^3)^2 + 1/48 * mean(z^4)^2}
Z_neg_entropy <- apply(Z,2,neg_entropy)
ic_sort <- sort(Z_neg_entropy,decreasing=TRUE,index.return=TRUE)$ix
ic_sort

# Plot the sorted neg-entropy and define the ICs sorted by neg-entropy
par(mfrow=c(1,1))
plot(Z_neg_entropy[ic_sort],type="b",col=color_1,pch=19,
     ylab="Neg-entropy",main="Neg-entropies",lwd=3)
```

After plotting, the negative entropies we can see that there are 4 ICs larger than the other ones.

```{r}
set.seed(10040485)
Z_ic_imp <- Z[,ic_sort]

# Plot the two ICs with largest neg-entropy
par(mfrow=c(1,1))
plot(Z_ic_imp[,1:2],pch=19,col=colors_X,
     xlab="First IC scores",ylab="Second IC scores",main="First two IC scores")

# Let's identify the points with large negative values of the first IC

which(Z_ic_imp[,1]<(-1))

# Let's identify the points with large positive values of the second IC

which(Z_ic_imp[,2]>(6))
```

With the first two scores, its is pointed out that some cars fueled by gas are outliers.

In order to see what IC can show the differentiation between two groups, the scatterplot matrix with the highest and lowest neg-entropy will be plotted.

```{r}
# Plot the ICs with the highest and lowest entropy
pairs(Z_ic_imp[,c(1:5,12:13)],col=colors_X,pch=19,
      main="ICs with the highest and lowest entropy")
```

```{r}
plot(Z_ic_imp[,12:13],pch=19,col=colors_X,
     xlab="5-th IC scores",ylab="1-th IC scores",main="Last two IC scores")
```

In often cases, the differentiation between two groups can be seen in the pairs with the lowest negative entropy as it is where the minimum values of the kurtosis are attained. However, as it is shown in both graphs, we do not obtained a clear separation in this case.

```{r}
plot(Z_ic_imp[,6:7],pch=19,col=colors_X,
      xlab="7-th IC scores",ylab="4-th IC scores")
```

Nevertheless, looking at the plot of the ICs we can see that the seventh IC had two different groups, so we decided to check them and we obtained that the formed groups are differentiated by the variable `fuel_type`.

The correlation between the data and the ICs is calculated:

```{r}
# Correlation data vs ICs
corrplot(cor(data_pca,Z_ic_imp),is.corr=T)
```

In this case, the IC7 is highly positively correlated with the `compressio-ratio` and that is why this IC is able to differentiate between diesel and gas cars. On the other hand, the IC3 is negatively correlated with `highway_mpg`.

Finally, we check the correlation between the PCA components and the ICs.

```{r}
# Correlation PCA vs ICs
corrplot(cor(prcomp(data_pca)$x,Z),is.corr=T)
```

As it can be seen, the correlations are not very high for the most part, which is the most common situation. However, there are some that could be highlighted, for example the negative correlation between the IC4 and PC13 and the positive correlation between IC7 and PC3.

## Factor analysis

```{r}
r <- 3
data_pca<-scale(data_pca)
p<-ncol(data_pca)
# Initial estimation of M and Sigma_nu
M_0 <- p1$rotation[,1:r] %*% diag(p1$sdev[1:r])
M_0
S_y <- cov(data_pca)
Sigma_nu_0 <- diag(diag(S_y - M_0 %*% t(M_0)))
Sigma_nu_0

# Estimation of M without varimax rotation
MM <- S_y - Sigma_nu_0
MM_eig <- eigen(MM)
MM_values <- MM_eig$values
MM_vectors <- MM_eig$vectors
M_1 <- MM_eig$vectors[,1:r] %*% diag(MM_eig$values[1:r])^(1/2)
M_1

# Final estimation of M and Sigma_nu after varimax rotation for interpretability
M <- varimax(M_1)
M <- loadings(M)[1:p,1:r]
M
Sigma_nu <- diag(diag(S_y - M %*% t(M)))
Sigma_nu

# Understanding the relationship between observable variables and factors
# First factor 
plot(1:p,M[,1],pch=19,col=color_1,xlab="",ylab="Loadings",main="Loadings for the first factor")
abline(h=0)
text(1:p,M[,1],labels=colnames(data_pca),pos=1,col=color_2,cex=0.75)
# Second factor 
## 4 is height and 9 is compressio_ratio
plot(1:p,M[,2],pch=19,col=color_1,xlab="",ylab="Loadings",main="Loadings for the second factor")
abline(h=0)
text(1:p,M[,2],labels=colnames(data_pca),pos=1,col=color_2,cex=0.75)
# Third factor  
# 8 is stroke
plot(1:p,M[,3],pch=19,col=color_1,xlab="",ylab="Loadings",main="Loadings for the third factor")
abline(h=0)
text(1:p,M[,3],labels=colnames(data_pca),pos=1,col=color_2,cex=0.75)

# Communalities
comM <- diag(M %*% t(M))
comM
names(comM) <- colnames(data_pca)
plot(1:p,sort(comM,decreasing=TRUE),pch=20,col=color_1,xlim=c(0,15),
     xlab="Variables",ylab="Communalities",main="Communalities")
text(1:p,sort(comM,decreasing=TRUE),labels=names(sort(comM,decreasing=TRUE)),
     pos=4,col=color_2,cex=0.75)

# Estimate the factor scores
Fact <- data_pca %*% solve(Sigma_nu) %*% M %*% solve(t(M) %*% solve(Sigma_nu) %*% M)
colnames(Fact) <- c("Factor 1","Factor 2","Factor 3")

# See that the factors are uncorrelated
pairs(Fact,pch=19,col=color_1)
corrplot(cor(Fact),order="hclust")

# Estimate the residuals

Nu <- data_pca - Fact %*% t(M)
corrplot(cor(Nu),order="hclust")
```

Watching the plots that represent the relationship between variables and factors, we obtain that the first factor symbolizes the smallness of the cars as the cars with higher values are the least large, least powerful and with less consumption in highgway (most miles per gallon). The second factor is an index of power and poor efficiency (compression_ratio, variable number 9, means efficiency). The third factor symbolizes tall cars with a short engine displacement.

If we analyze the interpretations of the biplots in the PCA and the interpretations of the factor analysis we find out that the relationship between the principal components and the variables is the same as the relationship of variables and factor but in the case of the first elements they present the opposite signs: if the PC1 is related with big and powerful cars, the first factor is an index of small and less powerful cars.

Then, in the communalities plot we can see that the variables that are better explained by the factors are `curb_weight`, `length`, `horsepower`, `engine_size`, `highway_mpg` and `wheel_base` and clearly the worst explained one is `compressio_ratio`. Afterwards, we compute the estimations of the factors' scores and with the plots we check that there is practically no correlation between them. Finally, with the representation of the correlation matrix of the residuals we find out that there exists some correlations that the factor model is not able to explain.

# Unsupervised classification

## Partitional clustering

```{r}
pairs(p1$x[,1:2],pch=19,main="The first two PCs")
```

These last two graphs are tricky. When the first component is represented in the x-axis (plot on the bottom left) it does not seem that there is an obvious differentiation between two groups. On the other hand, when the second component is on the x-axis it may look like there is an obvious distinctness between the group on the left with few components and the group on the right with a larger number of instances. Hereafter, it is explained how the clustering was carried out.


### K-means

First of all, it has to be pointed out that for this methods it is necessary to standarize the variables.

When we use the 'Within-cluster sum of squares' as a criteria to determine the optimal number of clusters, the result obtained is not clear at all as there is no evident 'elbow' in the plot.

```{r}
color_1 <- "deepskyblue2"
color_2 <- "darkorchid4"
color_3 <- "seagreen2"
color_4 <- "orange2"
data_nume<-scale(data_num)
fviz_nbclust(data_nume,kmeans,method="wss",k.max=10)
```

Therefore, we proceed to determine the optimal value of 'k' with the 'Average silhouette'. We obtained that the best option was using k=2 as it presented the highest average as it can be seen in the plot below.

```{r}
fviz_nbclust(data_nume,kmeans,method="silhouette",k.max=10)
```


Then we carry out the k-means approach and we represent the observations with the color of their respective group. We can observe that there is a cluster on the right with fewer instances than the one located on the left. Through the silhouette plot we can denote that all of the observations (except one) belong to the cluster whose mean is closer to that observation (because all the values are positive). That is a good indicative as the negative value means that the observation should be part of the other group because it is closer to the other mean. We also obtained that the average silhouette width for this method is 0.33.

```{r}
kmeans_X <- kmeans(data_nume,centers=2,iter.max=1000,nstart=100)
colors_kmeans_X <- c(color_1,color_2)[kmeans_X$cluster]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_kmeans_X,xlab="First PC",ylab="Second PC")
sil_kmeans_X <- silhouette(kmeans_X$cluster,dist(data_nume,"euclidean"))
plot(sil_kmeans_X,col=color_1)
```


### K-medoids

Another possibility for clustering is the k-medoids clustering which instead of means, uses medoids (center observation of the cluster) and instead of the the Euclidean distance it uses the Manhattan (or Gower) distance. There are different algorithms in the k-medoid method:

#### PAM (I)

PAM (Partition Around Medoids) is the standard k-medoids algorithm. The resultant clustering gives a more balanced partition as it can be seen in the first plot. But even if the average silhouette (0.35) is larger than the previous one, this option is worse as there are more negative values and therefore more wrongly assigned observations.

```{r}
pam_X <- pam(data_nume,k=2,metric="manhattan",stand=FALSE)
colors_pam_X <- c(color_1,color_2)[pam_X$cluster]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_pam_X,xlab="First PC",ylab="Second PC")
sil_pam_X <- silhouette(pam_X$cluster,dist(data_nume,method="manhattan"))
plot(sil_pam_X,col=color_1)
```


#### PAM (II)

Another option is to use the Gower distance with the PAM algorithm. We have obtained that adding the non-numerical variables as binary variables (through one-hot encoding) results on tacking on a lot of noise and therefore the result is less likeable. That is the reason why we use just the numeric data.

```{r}
#Just numeric data
X_Gower <- daisy(data_nume,metric="gower")
summary(X_Gower)
```

```{r}
X_Gower_mat <- as.matrix(X_Gower)
X_K <- matrix(NA,nrow=1,ncol=19)
for (i in 1:19){
  pam_X_Gower_mat <- pam(X_Gower_mat,k=i+1,diss=TRUE)
  X_K[i] <- pam_X_Gower_mat$silinfo$avg.width
}
plot(2:20,X_K,pch=19,col="deepskyblue2",xlab="Number of clusters",ylab="Average silhouette")
```

```{r}
pam_X_Gower_mat <- pam(X_Gower_mat,k=2,diss=TRUE)
colors_pam_2 <- c(color_1,color_2)[pam_X_Gower_mat$cluster]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_pam_2,xlab="First PC",ylab="Second PC")
sil_pam_X_Gower_mat <- silhouette(pam_X_Gower_mat$cluster,X_Gower_mat)
plot(sil_pam_X_Gower_mat,col=color_1)
summary(sil_pam_X_Gower_mat)
```

We get a similar result as the one obtained with the Manhattan distance: there are some negative values in the silhouette graph and therefore we still prefer the k-means method.

##### Adding factors

```{r, warning=FALSE}
data$symboling<-as.factor(data$symboling)
data_cat<-data[,c(1:8,14,15,17)]
data_cat <- one_hot(as.data.table(data_cat))
data_e<-cbind(data_nume,data_cat)
```

```{r, warning=FALSE}
X_Gower <- daisy(data_e,metric="gower")
summary(X_Gower)
```

```{r}
X_Gower_mat <- as.matrix(X_Gower)
X_K <- matrix(NA,nrow=1,ncol=19)
for (i in 1:19){
  pam_X_Gower_mat <- pam(X_Gower_mat,k=i+1,diss=TRUE)
  X_K[i] <- pam_X_Gower_mat$silinfo$avg.width
}
plot(2:20,X_K,pch=19,col="deepskyblue2",xlab="Number of clusters",ylab="Average silhouette")
which.max(X_K)+1
```


#### CLARA

The aim of CLARA (Clustering for Large Applications) is computational efficiency by running PAM to random sub-samples, not to improve PAM results. In the Silhouette plot we found that the average silhoutte is slightly larger than the previous ones but there are still some negative values, that is why we still prefering the k-means solution.

```{r}
clara_X <- clara(data_nume,k=2,metric="manhattan",stand=FALSE)
colors_clara_X <- c(color_1,color_2)[clara_X$cluster]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_clara_X,xlab="First PC",ylab="Second PC")
sil_clara_X <- silhouette(clara_X$cluster,dist(data_nume,method="manhattan"))
plot(sil_clara_X,col=color_1)
```


## Hierarchical

These are methods that do not require to fix K as they explore cluster solutions for every possible value of k. As it is possible to use mixed data (using the Gower distance) we have tried different options to find out if in our case it was better to use only numeric data or the mixed dataset.

### Aglomerative

In these cases, the algorithms start from k = n and then merges clusters based on the distances between them. There are different methods to compute the distance between a new cluster and the other clusters, these are called 'linkage methods' and are the following ones:

```{r}
man_dist_X <- daisy(data_nume,metric="manhattan",stand=F)
gower_dist <- daisy(data_e,metric="gower",stand=F)
```

#### Single linkage

##### Manhattan distance (numeric data)

```{r}
single_X <- hclust(man_dist_X,method="single")
plot(single_X,main="Single linkage",cex=0.8)
rect.hclust(single_X,k=2,border=color_1)
cl_single_X <- cutree(single_X,2)
colors_single_X <- c(color_1,color_2)[cl_single_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_single_X, xlab="First PC",ylab="Second PC")
sil_single_X <- silhouette(cl_single_X,man_dist_X)
plot(sil_single_X,col=color_1)
```

##### Gower distance (mixed data)

```{r}
single_X <- hclust(gower_dist,method="single")
plot(single_X,main="Single linkage",cex=0.8)
rect.hclust(single_X,k=2,border=color_1)
cl_single_X <- cutree(single_X,2)
colors_single_X <- c(color_1,color_2)[cl_single_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_single_X, xlab="First PC",ylab="Second PC")
sil_single_X <- silhouette(cl_single_X,man_dist_X)
plot(sil_single_X,col=color_1)
```

Normally this linkage method tends to create large clusters and this is what we have obtained with both approaches. The results are quite awful as we can see in both the representation of the dendogram does not look right and the plot of the Silhouette shows that there are a lot of observations wrongly assinged (negative values). These were not a good option.

#### Complete

##### Manhattan distance (numeric data)

```{r}
complete_X <- hclust(man_dist_X,method="complete")
plot(complete_X,main="Complete linkage",cex=0.8)
rect.hclust(complete_X,k=2,border=color_1)
cl_complete_X <- cutree(complete_X,2)
colors_complete_X <- c(color_1,color_2)[cl_complete_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_complete_X, xlab="First PC",ylab="Second PC")
sil_complete_X <- silhouette(cl_complete_X,man_dist_X)
plot(sil_complete_X,col=color_1)
```

In this case the dendogram looks better. As it was expected the groups are a little bit more balanced (still imbalanced) and we obtained the Silhouette still presents more negative values than the k-means method.

##### Gower distance (mixed data)

```{r}
complete_X <- hclust(gower_dist,method="complete")
plot(complete_X,main="Complete linkage",cex=0.8)
rect.hclust(complete_X,k=2,border=color_1)
cl_complete_X <- cutree(complete_X,2)
colors_complete_X <- c(color_1,color_2)[cl_complete_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_complete_X, xlab="First PC",ylab="Second PC")
sil_complete_X <- silhouette(cl_complete_X,man_dist_X)
plot(sil_complete_X,col=color_1)
```

Using the Gower distance the groups are also imbalanced as it can be seen in the dendogram and in the Silhouette even with a smaller average Silhouette, the results are similar to the previous approach. The conclusion is the same, k-means is still a better option.

#### Average

This method was expected to give an intermediate solution between the single and the complete linkage methods, and this is what we have seen in the plots.

##### Manhattan distance (numeric data)

```{r}
average_X <- hclust(man_dist_X,method="average")
plot(average_X,main="Average linkage",cex=0.8)
rect.hclust(average_X,k=2,border=color_1)
cl_average_X <- cutree(average_X,2)
colors_average_X <- c(color_1,color_2)[cl_average_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_average_X, xlab="First PC",ylab="Second PC")
sil_average_X <- silhouette(cl_average_X,man_dist_X)
plot(sil_average_X,col=color_1)
```

In this approach we have a more imbalanced result and eventhough the average silhouette is the largest, there are more negative values, what makes it a worse method.

##### Gower distance (mixed data)

```{r}
average_X <- hclust(gower_dist,method="average")
plot(average_X,main="Average linkage",cex=0.8)
rect.hclust(average_X,k=2,border=color_1)
cl_average_X <- cutree(average_X,2)
colors_average_X <- c(color_1,color_2)[cl_average_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_average_X, xlab="First PC",ylab="Second PC")
sil_average_X <- silhouette(cl_average_X,man_dist_X)
plot(sil_average_X,col=color_1)
```

With the Gower distance and the mixed dataset the clusters are more imbalanced and the result is even worse as there are more wrongly assigned observations.

#### Ward

##### Manhattan distance (numeric data)

```{r, message=FALSE, warning=FALSE}
ward_X <- hclust(man_dist_X,method="ward")
plot(ward_X,main="Ward linkage",cex=0.8)
rect.hclust(ward_X,k=2,border=color_1)
cl_ward_X <- cutree(ward_X,2)
colors_ward_X <- c(color_1,color_2)[cl_ward_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_ward_X,xlab="First PC",ylab="Second PC")
sil_ward_X <- silhouette(cl_ward_X,man_dist_X)
plot(sil_ward_X,col=color_1)
```

##### Gower distance (mixed data)

```{r}
ward_X <- hclust(gower_dist,method="ward")
plot(ward_X,main="Ward linkage",cex=0.8)
rect.hclust(ward_X,k=2,border=color_1)
cl_ward_X <- cutree(ward_X,2)
colors_ward_X <- c(color_1,color_2)[cl_ward_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_ward_X,xlab="First PC",ylab="Second PC")
sil_ward_X <- silhouette(cl_ward_X,man_dist_X)
plot(sil_ward_X,col=color_1)
```

As it was expected, the ward linkage method provides similar results to the k-means method. More balanced groups than the other linkage methods are found but the Silhouette plots show that there are more observations in clusters that they do not belong to than with the k-means method. Therefore, we still prefer the k-means approach.

An interesting point found out in this section is that in general with the Gower distance, lower average silhouettes are obtained (that might be bacuse the group means are closer with this distance) and more observations are wrongly assigned. Generally Manhattan distance finds better solutions.

### Divisive (DIANA)

In the Divisive Analysis Clustering (DIANA) all the observations belong to a single cluster, and at each step the cluster with the largest diameter is divided into two clusters and so on until there is a cluster for each observation. 

```{r}
diana_X <- diana(data_nume,metric="manhattan")
par(mfrow=c(1,2))
plot(diana_X,main="DIANA")
# The heights here are the diameters of the clusters before splitting
rect.hclust(diana_X,k=2,border=color_1)
cl_diana_X <- cutree(diana_X,2)
colors_diana_X <- c(color_1,color_2)[cl_diana_X]
par(mfrow=c(1,2))
plot(p1$x[,1:2],pch=19,col=colors_diana_X,xlab="First PC",ylab="Second PC")
sil_diana_X <- silhouette(cl_diana_X,man_dist_X)
plot(sil_diana_X,col=color_1)
```

This approach presents the best results until this moment. With the dendogram and the PC's plot we can see that the groups are balanced (as in k-means) and in the Silhouette plot no negative value is present, therefore this solution is the best one yet.

## Model based (M-CLUST)

This approach does not use distances but probability to obtain results. Asuming that the observations are generated by different distributions with certain probabilities, these observations are assigned to different clusters according to the Bayes Theorem. M-clust is the most widely used model-based clustering method and it works like this:

```{r, warning=FALSE, message=FALSE}
BIC_X <- mclustBIC(p1$x[,1:2],G=1:5)
plot(BIC_X)
Mclust_X <- Mclust(p1$x[,1:2],x=BIC_X)
summary(Mclust_X)
# Plot the groups
par(mfrow=c(1,2))
plot(Mclust_X,what="classification")
# Plot the estimated densities
plot(Mclust_X,what="density")
# Plot the estimated probabilities of the observations for each cluster
colors_Mclust_X <- c(color_1,color_2,color_3)[Mclust_X$classification]
par(mfrow=c(2,2))
plot(1:nrow(data_num),Mclust_X$z[,1],pch=19,col=colors_Mclust_X,main="Cluster 1",
     xlab="Cancer cell",ylab="Probability of cluster 1")
plot(1:nrow(data_num),Mclust_X$z[,2],pch=19,col=colors_Mclust_X,main="Cluster 2",
     xlab="Cancer cell",ylab="Probability of cluster 2")
plot(1:nrow(data_num),Mclust_X$z[,3],pch=19,col=colors_Mclust_X,main="Cluster 3",
     xlab="Cancer cell",ylab="Probability of cluster 3")

# Plot the points with uncertainty
#par(mfrow=c(1,1))
plot(Mclust_X,what="uncertainty")
```

In the first plot we can see that the model with the best results is the model with 3 clusters in which the covariance matrices are diagonal, have different volume and shape (VVI). With the two following graphs we can observe the distribution of the clusters and their diagonal densities. In the last plots the probabilities of belonging to each cluster are represented and also the observations that provoke more indecision about its assingment are shown.

```{r}
sil_mclust_X <- silhouette(Mclust_X$classification,dist(data_nume,method="manhattan"))
plot(sil_mclust_X,col=color_1)
```

Finally, we computed the Silhouette plot of this method to find out that this approach is not as good as the previous methods because there are more and larger negative values. This means that there are more points that are wrongly  assigned and with a larger error magnitude.

## Density

Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is an algorithm that uses distances (normally Euclidean) between observations to cluster. Two parameters have to be set, $\epsilon$ which is a measure of closeness and $MP$ which is the minimum number of points to form a cluster. In this case, the fixed values are $1.05$ for $\epsilon$ and $10$ for $MP$ because they present the best results.

```{r, warning=FALSE, message=FALSE}
minPts <- 10
par(mfrow=c(1,2))
kNNdistplot(p1$x[,1:2],k=minPts-1)
abline(h=1.05,lty=2,lwd=2,col=color_1)
dbscan_Z <- dbscan(p1$x[,1:2],eps=1.05,minPts=minPts)
dbscan_Z
colors_dbscan_Z <- c(color_1,color_2,color_3,color_4)[dbscan_Z$cluster+1]
plot(p1$x[,1:2],pch=19,col=colors_dbscan_Z,xlab="First PC",ylab="Second PC")
```

In the first plot we can see why $\epsilon$ is $1.05$ as the first elbow of the graph is in that point. In the second plot it is represented that this method with these parameters finds that the best option is to form a big central cluster and that the rest of observations (colored in blue) are noise.

```{r}
sil_dbscan_X <- silhouette(dbscan_Z$cluster,dist(data_nume,method="manhattan"))
plot(sil_dbscan_X,col=color_1)
```

To measure the goodness of this method, we computed the Silhouette plot and the result is worse than the results from the DIANA algorithm. The average Silhouette is similar but there are several points that do not belong to the cluster that they were assigned, therefore we do not prefer the latter method.

## Conclusion of the unsupervised analysis

As it was mentioned before, the best results were obtained with the DIANA method as no negative value was present in the Silhouette plot and therefore no wrong assingnation took place.


# Supervised classification

The surpervised classification will be carried out with the `fuel_type` variable, which is heavily imbalanced so the results may not be reliable.

Moreover, this part of the project will be carried out only the numerical values will be taken into account because the categorical values of the 20 cars fueled by diesel are almost constant which can cause problems with the classifications based on the Bayes Theorem. In addition, for the KNN classification, the Euclidean distance will be used, and it does not work well with categorical values.


```{r}
options(digits=4)

color_3 <- "seagreen2"
color_4 <- "indianred2"
        
# Define the data matrix and the response vector

X <- data_num[,!names(data_num) %in% c("Class")]
Y <- data$fuel_type

c_gas <- sum(Y=="gas")
c_diesel <- sum(Y=="diesel")
c(c_gas,c_diesel)

n = nrow(data_num)
pr_gas <- c_gas/n
pr_diesel <- c_diesel/n
c(pr_gas,pr_diesel)
```

## Visual analysis

An initial graphical analysis is going to be done in order to identify how difficult it will be to solve the problem.

### Parallel Coordinates Plot

```{r}
colors_Y <- c(color_1,color_2)[1*(Y=="diesel")+1]
parcoord(X,col=colors_Y,var.label=F,main="PCP")
```

In this graph we can see that many of the variables do not allow to distinguish well the groups. However, there is one that clearly differenciates the data, which is `compressio_ratio` as we saw in the previous sections

```{r}
X_skewness <- apply(X,2,skewness)
sort(X_skewness,decreasing=TRUE)
plot(1:ncol(X),apply(X,2,skewness),type="h",pch=19,col=color_1,xlab="Variables",
     ylab="Skewness coefficients",main="Skewness coefficients")
```

We can see that the largest skewness coefficient is 2.59 and the minimum is -0.63, so not all the variables are very skewed.

The variables which are skewed are:
- compressio_ratio      
- engine_size            
- price       
- horsepower 


### PCA

```{r}
X_pcs <- prcomp(X,scale=TRUE)
summary(X_pcs)
```

Three components are necessary to explain around 76% of the variability and using four components will explain 83% of it.

```{r}
# Make a plot of the first two PCs

plot(X_pcs$x[,1:2],pch=20,col=colors_Y,xlim=c(-4.5,5.5),ylim=c(-7,5),
     main="First two PCs")
```

This plot shows that there are two clearly differenciated groups with the first two components.

## Methods

We divide the data into the train and test partitions.

```{r}
set.seed(485)

# Obtain a training and a test sample: Take at random the 70% of the observations as the training sample and 
# the other 30% of the observations as the test sample

n_train <- floor(.7*n)
n_test <- n - n_train
c(n_train,n_test)

# Obtain the indices of the observations in both data sets at random

i_train <- sort(sample(1:n,n_train))
length(i_train)
head(i_train)
i_test <- setdiff(1:n,i_train)
length(i_test)
head(i_test)

# Obtain the training data matrix and the associated response vector

X_train <- X[i_train,]
Y_train <- Y[i_train]


# Obtain the test data matrix and the associated response vector

X_test <- X[i_test,]
Y_test <- Y[i_test]


# Which are the proportions of diesel and gas cars in the training and test sample

sum(Y_train=="gas")/n_train
sum(Y_train=="diesel")/n_train
sum(Y_test=="gas")/n_test
sum(Y_test=="diesel")/n_test

# There similar to the original data
```


### k-nearest neighbors (kNN)

The KNN method will be using the Euclidean distance, so the data will b scaled in order to obtained the best results posible.
First, we will compute the LOOCV Error Rate (LER) in order to estimate the optimal value for k.

```{r}
# Scale the data
scaled_X_train <- scale(X_train,center=F)
scaled_X_test <- scale(X_test,center=F)

# We take k ranging from 3 to 40 as the sample size is big enough and estimate the LOOCV error rate

LER <- rep(NA,37)
for (i in 3 : 40){
  print(i)
  knn_output <- knn.cv(scaled_X_train,Y_train,k=i)
  LER[i] <- 1 - mean(knn_output==Y_train)
}
plot(1:40,LER,pch=20,col=color_1,type="b",
     xlab="k",ylab="LER",main="LER")

# Take k as the one that gives the minimum LOOCV error rate

(k <- which.min(LER))
```

The optimal values for K is 3.

```{r}
# Classify the responses in the test data set with the k selected

knn_Y_test <- knn(scaled_X_train,scaled_X_test,Y_train,k=k,prob=T)
head(knn_Y_test)

# Number of cars classified in each group

summary(knn_Y_test)

# Confusion table

table(Y_test,knn_Y_test)

# Obtain the Test Error Rate (TER)

knn_TER <- mean(Y_test!=knn_Y_test)
knn_TER

# Estimated probabilities of the classifications made for the winner group

prob_knn_Y_test <- attributes(knn_Y_test)$prob
head(prob_knn_Y_test)

# Make a plot of the probabilities of the winner group
# In green, good classifications, in red, wrong classifications

colors_errors <- c(color_4,color_3)[1*(Y_test==knn_Y_test)+1]
plot(1:n_test,prob_knn_Y_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Winning probabilities",main="Winning probabilities")

# Note that there have been some ties and that observations at the same distance to the observation 
# to be classified have been included
```


### Methods based on the Bayes Theorem.

#### Linear Discriminant Analysis (LDA).

```{r}
lda_train <- lda(Y_train~.,data=as.data.frame(X_train))

# Estimated prior probabilities for the two groups

lda_train$prior

# Estimated sample mean vectors

t(lda_train$means)

# Classify the observations in the test sample

lda_test <- predict(lda_train,newdata=as.data.frame(X_test))

# The vector of classifications made can be found here

lda_Y_test <- lda_test$class
lda_Y_test

# Number of cars classified in each group

table(lda_Y_test)

# Contingency table with good and bad classifications

table(Y_test,lda_Y_test)

# Test Error Rate (TER)

lda_TER <- mean(Y_test!=lda_Y_test)
lda_TER

# Conditional probabilities of the classifications made with the test sample

prob_lda_Y_test <- lda_test$posterior
head(prob_lda_Y_test)

# In green, good classifications, in red, wrong classifications

colors_errors <- c(color_4,color_3)[1*(Y_test==lda_Y_test)+1]
plot(1:n_test,prob_lda_Y_test[,1],col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of diesel",
     main="Probabilities of gas (LDA)")
abline(h=0.5)
```

#### Quadratic Discriminant Analysis (QDA).

```{r}
qda_train <- qda(Y_train~.,data=as.data.frame(X_train))

eigen(cov(X_train[Y_train=="diesel",]))$values

summary(X_train[Y_train=="diesel",])

# Therefore, we apply qda excluding this columns of all the data sets

qda_train <- qda(Y_train ~ .,data=as.data.frame(X_train))

# Estimated unconditional probabilities

qda_train$prior

# Estimated sample mean vectors

t(qda_train$means)

# Classify the observations in the test sample

qda_test <- predict(qda_train,newdata=as.data.frame(X_test))

# The vector of classifications made can be found here

qda_Y_test <- qda_test$class
qda_Y_test

# Number of cars classified in each group

table(qda_Y_test)

# Contingency table with good and bad classifications

table(Y_test,qda_Y_test)

# Test Error Rate (TER)

qda_TER <- mean(Y_test!=qda_Y_test)
qda_TER

# Conditional probabilities of the classifications made with the test sample

prob_qda_Y_test <- qda_test$posterior
head(prob_qda_Y_test)

# In green, good classifications, in red, wrong classifications

plot(1:n_test,prob_qda_Y_test[,1],col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of gas",
     main="Probabilities of gas (QDA)")
abline(h=0.5)
```


#### Naive Bayes (NB).

```{r, warning=FALSE}
nb_train <- gaussian_naive_bayes(X_train,Y_train)

# Estimated unconditional probabilities

nb_train$prior

# Estimated sample mean vectors

t(nb_train$params$mu)

t(nb_train$params$sd)

# Classify the observations in the test sample

nb_test <- predict(nb_train,newdata=as.matrix(X_test),type="prob")

# The vector of classifications made can be found here

nb_Y_test <- c("diesel","gas")[apply(nb_test,1,which.max)]
nb_Y_test

table(nb_Y_test)

# Contingency table with good and bad classifications

table(Y_test,nb_Y_test)

# Test Error Rate (TER)

nb_TER <- mean(Y_test!=nb_Y_test)
nb_TER

# Conditional probabilities of the classifications made with the test sample

prob_nb_Y_test <- nb_test
head(prob_nb_Y_test)

# In green, good classifications, in red, wrong classifications

plot(1:n_test,prob_nb_Y_test[,1],col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of gas",
     main="Probabilities of gas (Naive Bayes)")
abline(h=0.5)
```


### Logistic regression (LR).

```{r}
lr_train <- multinom(Y_train~.,data=as.data.frame(X_train))

# Have a look at the estimated coefficients and their standard errors

summary(lr_train)

# To see which are the most significant coefficients, we use the t-test that is computed next

(t_test_lr_train <- summary(lr_train)$coefficients/summary(lr_train)$standard.errors)

# Sort the absolute value of the t-test in decreasing order

sort(abs(t_test_lr_train),decreasing=TRUE)
```

The most relevant variables seem to be the `bore` and `stroke`.

There are a few variables, which are not significant such as the `engine_size` and the `price`.

```{r}
# Classify the responses in the test data set and estimate the test error rate

lr_test <- predict(lr_train,newdata=as.data.frame(X_test))
head(lr_test)

# Number of cars classified in each group

summary(lr_test)

# Table with good and bad classifications

table(Y_test,lr_test)

# Obtain the Test Error Rate (TER)

lr_TER <- mean(Y_test!=lr_test)
lr_TER

# Obtain the probabilities of gas

prob_lr_test <- 1 - predict(lr_train,newdata=X_test,type ="probs")
head(prob_lr_test)

# In green, good classifications, in red, wrong classifications

plot(1:n_test,prob_lr_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of gas",
     main="Probabilities of gas (Logistic Regression)")
abline(h=0.5)
```


## Conclusions

Looking at the obtained results with the supervised classification, all the methods classify perfectly the observations of the test sample. Therefore, there would be no difference in using one method or another.

However, these errors are not reliable as the response variable that has been used, `fuel_type`, makes the data heavily imbalanced. Moreover, the amount of observations of the diesel cars is only 20, so there are not sufficient cars in order to train and test the model correctly. This problem could have been solved with oversampling but in this case, it did not work well, which would be due to the homogeneity of these cars.
